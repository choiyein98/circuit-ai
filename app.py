import streamlit as st
import cv2
import numpy as np
from ultralytics import YOLO
import math
from PIL import Image
from collections import defaultdict
import gc

# ==========================================
# [ì„¤ì •] BrainBoard V66: Spatial Matching
# ==========================================
st.set_page_config(page_title="BrainBoard V66: Spatial", layout="wide")

REAL_MODEL_PATH = 'best(3).pt' 
MODEL_SYM_PATH = 'symbol.pt'

# ==========================================
# [Helper Functions]
# ==========================================
def resize_image_smart(image, max_size=1024):
    h, w = image.shape[:2]
    if max(h, w) > max_size:
        scale = max_size / max(h, w)
        new_w, new_h = int(w * scale), int(h * scale)
        return cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
    return image

def get_center(box):
    return ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)

def calculate_iou(box1, box2):
    x1 = max(box1[0], box2[0]); y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2]); y2 = min(box1[3], box2[3])
    inter = max(0, x2 - x1) * max(0, y2 - y1)
    union = ((box1[2]-box1[0])*(box1[3]-box1[1])) + ((box2[2]-box2[0])*(box2[3]-box2[1])) - inter
    return inter / union if union > 0 else 0

def normalize_name(name):
    name = name.lower()
    if 'res' in name: return 'resistor'
    if 'cap' in name: return 'capacitor'
    if 'wire' in name: return 'wire' # ì™€ì´ì–´ëŠ” ë¬´ì‹œí•  ì˜ˆì •
    if any(x in name for x in ['source', 'batt', 'volt', 'vdc']): return 'source'
    return name

def solve_overlap(parts):
    if not parts: return []
    parts.sort(key=lambda x: x.get('conf', 0), reverse=True)
    final = []
    for curr in parts:
        is_dup = False
        for k in final:
            iou = calculate_iou(curr['box'], k['box'])
            if iou > 0.4: is_dup = True; break
        if not is_dup: final.append(curr)
    return final

# ==========================================
# [Core Logic] ìœ„ì¹˜ ê¸°ë°˜ ì‹œí€€ìŠ¤ ì¶”ì¶œ
# ==========================================
def extract_spatial_sequence(parts, image_width):
    """
    ë¶€í’ˆì„ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì •ë ¬í•˜ê³ , 
    Xì¢Œí‘œê°€ ë¹„ìŠ·í•˜ë©´ ê°™ì€ 'ë‹¨ê³„(Stage)'ë¡œ ë¬¶ìŠµë‹ˆë‹¤.
    """
    # 1. Xì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    sorted_parts = sorted(parts, key=lambda x: x['center'][0])
    
    sequence = []
    current_stage = []
    
    if not sorted_parts: return []

    # ì²« ë²ˆì§¸ ë¶€í’ˆ
    current_stage.append(sorted_parts[0])
    last_x = sorted_parts[0]['center'][0]
    
    # 2. ê·¸ë£¹í™” (Xì¢Œí‘œ ì°¨ì´ê°€ ì´ë¯¸ì§€ ë„ˆë¹„ì˜ 15% ì´ë‚´ë©´ ê°™ì€ ê·¸ë£¹)
    threshold = image_width * 0.15 
    
    for i in range(1, len(sorted_parts)):
        curr = sorted_parts[i]
        curr_x = curr['center'][0]
        
        if abs(curr_x - last_x) < threshold:
            # ê°™ì€ ê·¸ë£¹ (ì˜ˆ: ë³‘ë ¬ ë°°ì¹˜)
            current_stage.append(curr)
        else:
            # ìƒˆë¡œìš´ ê·¸ë£¹ (ë‹¤ìŒ ë‹¨ê³„)
            # í˜„ì¬ ê·¸ë£¹ ë‚´ì—ì„œëŠ” Yì¢Œí‘œ(ìœ„->ì•„ë˜)ë¡œ ì •ë ¬
            current_stage.sort(key=lambda x: x['center'][1])
            sequence.append(current_stage)
            
            # ì´ˆê¸°í™”
            current_stage = [curr]
            last_x = curr_x
            
    # ë§ˆì§€ë§‰ ê·¸ë£¹ ì¶”ê°€
    if current_stage:
        current_stage.sort(key=lambda x: x['center'][1])
        sequence.append(current_stage)
        
    return sequence

def format_sequence(seq):
    """ì‚¬ëŒì´ ë³´ê¸° ì¢‹ì€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    formatted = []
    for stage in seq:
        names = [p['name'] for p in stage]
        if len(names) > 1:
            formatted.append(f"[{' & '.join(names)}]") # ë³‘ë ¬/ê°™ì€ìœ„ì¹˜
        else:
            formatted.append(names[0])
    return " â†’ ".join(formatted)

# ==========================================
# [Analysis 1] Schematic
# ==========================================
def analyze_schematic(img, model):
    img = resize_image_smart(img)
    h, w, _ = img.shape
    results = model.predict(source=img, save=False, conf=0.05, verbose=False)
    
    raw_parts = []
    for box in results[0].boxes:
        raw_name = model.names[int(box.cls[0])]
        norm_name = normalize_name(raw_name)
        if norm_name == 'wire' or norm_name == 'leg': continue # ìœ„ì¹˜ ë¹„êµì—ì„  ì œì™¸
        
        coords = box.xyxy[0].tolist()
        raw_parts.append({'name': norm_name, 'box': coords, 'center': get_center(coords), 'conf': float(box.conf[0])})

    parts = solve_overlap(raw_parts)

    # ì „ì› ë³´ì • (ì—†ìœ¼ë©´ ì œì¼ ì™¼ìª½)
    if parts and not any(p['name'] == 'source' for p in parts):
         leftmost = min(parts, key=lambda p: p['center'][0])
         leftmost['name'] = 'source'

    # ì‹œê°í™”
    for p in parts:
        x1, y1, x2, y2 = map(int, p['box'])
        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)
        cv2.putText(img, p['name'], (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

    sequence = extract_spatial_sequence(parts, w)
    return img, {'parts': parts, 'sequence': sequence, 'seq_str': format_sequence(sequence)}

# ==========================================
# [Analysis 2] Real Board
# ==========================================
def analyze_real(img, model):
    img = resize_image_smart(img)
    h, w, _ = img.shape
    
    res = model.predict(source=img, conf=0.10, verbose=False)
    raw_parts = []
    
    for b in res[0].boxes:
        raw_name = model.names[int(b.cls[0])]
        norm_name = normalize_name(raw_name)
        
        # ìœ„ì¹˜ ëŒ€ì¡° ë°©ì‹ì´ë¯€ë¡œ wire, leg ë“±ì€ ë¬´ì‹œí•˜ê³  ì£¼ìš” ë¶€í’ˆë§Œ ë´…ë‹ˆë‹¤
        if norm_name == 'wire' or norm_name == 'leg': continue
        if 'breadboard' in raw_name: continue
        
        conf = float(b.conf[0])
        if norm_name == 'resistor' and conf < 0.25: continue
        if norm_name == 'capacitor' and conf < 0.15: continue
        
        coords = b.xyxy[0].tolist()
        raw_parts.append({'name': norm_name, 'box': coords, 'center': get_center(coords), 'conf': conf})

    parts = solve_overlap(raw_parts)

    # ì „ì›(Source)ì´ ì—†ìœ¼ë©´ ì™€ì´ì–´ ìœ„ì¹˜ë¡œ ì¶”ì •í•˜ì§€ ì•Šê³ , 
    # ê·¸ëƒ¥ ì—†ìœ¼ë©´ ì—†ëŠ”ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤ (ìœ„ì¹˜ ëŒ€ì¡°ë‹ˆê¹Œ ì •í™•í•œ ë¶€í’ˆ ì¸ì‹ì´ ì¤‘ìš”)
    
    # ì‹œê°í™”
    for p in parts:
        color = (0, 255, 0)
        if p['name'] == 'source': color = (0, 255, 255)
        x1, y1, x2, y2 = map(int, p['box'])
        cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)
        cv2.putText(img, p['name'].upper(), (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    sequence = extract_spatial_sequence(parts, w)
    return img, {'parts': parts, 'sequence': sequence, 'seq_str': format_sequence(sequence)}

# ==========================================
# [Main UI]
# ==========================================
st.title("ğŸ§  BrainBoard V66: Spatial Matcher")
st.markdown("### ğŸ“ ë¶€í’ˆ ìœ„ì¹˜ ë° ë°°ì¹˜ ìˆœì„œ ë¹„êµ (Simple & Robust)")

@st.cache_resource
def load_models():
    gc.collect()
    return YOLO(REAL_MODEL_PATH), YOLO(MODEL_SYM_PATH)

try:
    model_real, model_sym = load_models()
    st.sidebar.success("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
except: st.stop()

col1, col2 = st.columns(2)
ref_file = col1.file_uploader("1. íšŒë¡œë„", type=['jpg', 'png', 'jpeg'])
tgt_file = col2.file_uploader("2. ì‹¤ë¬¼ ì‚¬ì§„", type=['jpg', 'png', 'jpeg'])

if ref_file and tgt_file:
    ref_image = Image.open(ref_file)
    tgt_image = Image.open(tgt_file)
    ref_cv = cv2.cvtColor(np.array(ref_image), cv2.COLOR_RGB2BGR)
    tgt_cv = cv2.cvtColor(np.array(tgt_image), cv2.COLOR_RGB2BGR)

    if st.button("ğŸš€ ìœ„ì¹˜/ìˆœì„œ ë¹„êµ ì‹¤í–‰"):
        gc.collect()
        with st.spinner("ë¶€í’ˆì˜ ë°°ì¹˜ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            
            res_ref_img, ref_data = analyze_schematic(ref_cv.copy(), model_sym)
            res_tgt_img, tgt_data = analyze_real(tgt_cv.copy(), model_real)

            # 1. BOM Check
            st.subheader("1. ë¶€í’ˆ ê°œìˆ˜ í™•ì¸")
            ref_counts = defaultdict(int)
            tgt_counts = defaultdict(int)
            for p in ref_data['parts']: ref_counts[p['name']] += 1
            for p in tgt_data['parts']: tgt_counts[p['name']] += 1
            
            all_keys = set(ref_counts.keys()) | set(tgt_counts.keys())
            bom_match = True
            bom_data = []
            for k in all_keys:
                r = ref_counts[k]; t = tgt_counts[k]
                status = "âœ… ì¼ì¹˜" if r == t else "âŒ ë¶ˆì¼ì¹˜"
                bom_data.append({"ë¶€í’ˆëª…": k.upper(), "íšŒë¡œë„": r, "ì‹¤ë¬¼": t, "ìƒíƒœ": status})
                if r != t: bom_match = False
            st.table(bom_data)

            # 2. Sequence Check (í•µì‹¬)
            st.subheader("2. ë°°ì¹˜ ìˆœì„œ ë¹„êµ (Left -> Right)")
            
            ref_seq_str = ref_data['seq_str']
            tgt_seq_str = tgt_data['seq_str']
            
            st.info(f"ğŸ“œ **íšŒë¡œë„ ìˆœì„œ:** {ref_seq_str}")
            st.info(f"ğŸ“¸ **ì‹¤ë¬¼ ë°°ì¹˜:** {tgt_seq_str}")
            
            # ë‹¨ìˆœ ë¬¸ìì—´ ë¹„êµê°€ ì•„ë‹ˆë¼ ë‹¨ê³„ë³„ êµ¬ì„±ìš”ì†Œ ë¹„êµ
            ref_seq = ref_data['sequence']
            tgt_seq = tgt_data['sequence']
            
            is_seq_match = True
            
            if len(ref_seq) != len(tgt_seq):
                st.error("âš ï¸ **ë°°ì¹˜ ë‹¨ê³„(Column) ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤.** ë¶€í’ˆì´ ë„ˆë¬´ ëª°ë ¤ìˆê±°ë‚˜ í¼ì ¸ìˆì§€ ì•Šì€ì§€ í™•ì¸í•˜ì„¸ìš”.")
                is_seq_match = False
            else:
                for i, (r_stage, t_stage) in enumerate(zip(ref_seq, tgt_seq)):
                    r_names = sorted([p['name'] for p in r_stage])
                    t_names = sorted([p['name'] for p in t_stage])
                    
                    if r_names == t_names:
                        st.success(f"âœ… Step {i+1}: {r_names} ë°°ì¹˜ ì¼ì¹˜")
                    else:
                        st.error(f"âŒ Step {i+1}: ë¶ˆì¼ì¹˜ (íšŒë¡œë„:{r_names} vs ì‹¤ë¬¼:{t_names})")
                        is_seq_match = False

            if is_seq_match and bom_match:
                st.success("ğŸ‰ **ì™„ë²½í•©ë‹ˆë‹¤! ë¶€í’ˆì˜ ì¢…ë¥˜, ê°œìˆ˜, ë°°ì¹˜ ìˆœì„œê°€ ëª¨ë‘ ì¼ì¹˜í•©ë‹ˆë‹¤.**")
                st.balloons()
            
            st.image(cv2.cvtColor(res_ref_img, cv2.COLOR_BGR2RGB), caption="íšŒë¡œë„ ë°°ì¹˜ ë¶„ì„", use_column_width=True)
            st.image(cv2.cvtColor(res_tgt_img, cv2.COLOR_BGR2RGB), caption="ì‹¤ë¬¼ ë°°ì¹˜ ë¶„ì„ (ì™€ì´ì–´ ë¬´ì‹œ)", use_column_width=True)
            
            del res_ref_img, res_tgt_img
            gc.collect()
